{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ab6f03d",
   "metadata": {},
   "source": [
    "# TP : Clustering avec DBSCAN\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### üåü Objectifs du TP\n",
    "- Comprendre le fonctionnement de l'algorithme DBSCAN.\n",
    "- L'utiliser pour regrouper des donn√©es sans conna√Ætre √† l'avance le nombre de clusters.\n",
    "- Comparer ses r√©sultats √† ceux de K-Means.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Th√©orie : DBSCAN\n",
    "\n",
    "### üîç Qu'est-ce que DBSCAN ?\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) est un algorithme de clustering bas√© sur la densit√© des points.\n",
    "Il est capable de trouver des clusters de formes arbitraires et de reconna√Ætre les points isol√©s comme du bruit.\n",
    "\n",
    "### ‚öôÔ∏è Param√®tres importants :\n",
    "- `eps` : **distance maximale** pour qu‚Äôun point soit consid√©r√© comme voisin.\n",
    "- `min_samples` : **nombre minimum de points** dans un voisinage pour former un cluster.\n",
    "\n",
    "### üßπ Types de points :\n",
    "- Point **central** : assez de voisins proches.\n",
    "- Point **bordure** : proche d‚Äôun point central mais sans assez de voisins.\n",
    "- Point **bruit** : trop √©loign√© des autres.\n",
    "\n",
    "### üîç Attribution de label :\n",
    "Le mod√®le DBSCAN attribue √† chaque point :\n",
    "\n",
    "- soit un **num√©ro de cluster** (ex. 0, 1, 2‚Ä¶),\n",
    "\n",
    "- soit -1 pour les points consid√©r√©s comme du **bruit** (outliers).\n",
    "\n",
    "\n",
    "\n",
    "## 3. Avantages et inconv√©nients de DBSCAN\n",
    "\n",
    "### ‚úÖ Avantages :\n",
    "- Identifie des clusters de **forme irr√©guli√®re**.\n",
    "- G√®re automatiquement les **points aberrants** (bruit).\n",
    "- Pas besoin de pr√©ciser le nombre de clusters.\n",
    "\n",
    "### ‚ùå Inconv√©nients :\n",
    "- Sensible au choix des param√®tres `eps` et `min_samples`.\n",
    "- Moins efficace si la densit√© varie beaucoup entre clusters.\n",
    "\n",
    "### ‚ÑπÔ∏è Note p√©dagogique ‚Äì Post-traitement possible :\n",
    "Une fois les clusters d√©tect√©s par DBSCAN, il est parfois utile de :\n",
    "\n",
    "‚úÖ **Supprimer les clusters trop petits** consid√©r√©s comme peu significatifs.\n",
    "\n",
    "üîÅ Ou **tenter de les fusionner** avec des clusters proches.\n",
    "\n",
    "üí° *Ces √©tapes rel√®vent du post-traitement du clustering,\n",
    "elles sont utiles mais ne sont pas abord√©es dans ce TP.*\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Exp√©rimentation sur un jeu de donn√©es simul√©\n",
    "\n",
    "### üìÅ 4.1 G√©n√©ration de donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de0bb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des biblioth√®ques\n",
    "!pip install pandas numpy scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6081b505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "X, y = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50)\n",
    "plt.title(\"Donn√©es simul√©es (formes en croissant)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32de5027",
   "metadata": {},
   "source": [
    "### üîå 4.2 Application de DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71537c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
    "labels = dbscan.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='plasma', s=50)\n",
    "plt.title(\"Clustering DBSCAN\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba0de44",
   "metadata": {},
   "source": [
    "### üìä 4.3 Comparaison avec K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a7ddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "kmeans.fit(X)\n",
    "k_labels = kmeans.labels_\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=k_labels, cmap='viridis', s=50)\n",
    "plt.title(\"Clustering K-Means\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3fb066",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìò 5. Estimation automatique des param√®tres DBSCAN\n",
    "\n",
    "Dans cette section, nous allons estimer automatiquement les param√®tres `min_samples` et `eps` √† partir des donn√©es. \n",
    "\n",
    "**<u>Important</u>** : une r√®gle *empirique* indique une fourchette entre **2 et 5 * nombre de crit√®res** (3 * nombre de crit√®res s'ils sont nombreux).\n",
    "\n",
    "\n",
    "### üîÑ 5.1 D√©termination du couple eps / min_samples :\n",
    "\n",
    "Ce code permet de tester plusieurs combinaisons de param√®tres pour l‚Äôalgorithme DBSCAN.\n",
    "\n",
    "Il affiche un tableau r√©capitulatif des valeurs estim√©es pour :\n",
    "\n",
    "- min_samples (nombre minimum de voisins)\n",
    "\n",
    "- eps (distance maximale entre voisins)\n",
    "\n",
    "Ensuite, il applique DBSCAN avec chaque couple (min_samples, eps)\n",
    "\n",
    "Il affiche :\n",
    "\n",
    "- le **nombre de clusters d√©tect√©s**\n",
    "\n",
    "- le **nombre de points** consid√©r√©s comme **bruit**\n",
    "\n",
    "- une visualisation claire des r√©sultats\n",
    "\n",
    "üéØ L‚Äôobjectif est de comparer les r√©sultats pour choisir le **meilleur couple de param√®tres**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d455f297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Calcul de la dimension n (nombre de crit√®res ici)\n",
    "dimension = X.shape[1]\n",
    "\n",
    "# R√®gle empirique pour d√©terminer l'intervalle de min_samples (seulement 2 dimensions ici)\n",
    "min_samples_range = range(2, 5 * dimension)\n",
    "eps_results = []  # Stocker les distances moyennes des plus grands √©carts visuels\n",
    "\n",
    "# Test pour chaque valeur de min_samples : le KNN permet d'estimer l'EPS\n",
    "for min_samples in min_samples_range:\n",
    "    neighbors = NearestNeighbors(n_neighbors=min_samples).fit(X)\n",
    "    distances, _ = neighbors.kneighbors(X)\n",
    "    k_distances = np.sort(distances[:, min_samples - 1])\n",
    "\n",
    "    # En guise d'estimation simple, on prend la valeur au 90e percentile\n",
    "    eps_est = np.percentile(k_distances, 90)\n",
    "    eps_results.append((min_samples, round(eps_est, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e015ca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Cr√©ation du tableau √† partir des r√©sultats\n",
    "eps_df = pd.DataFrame(eps_results, columns=[\"min_samples\", \"eps estim√©\"])\n",
    "print(eps_df)\n",
    "\n",
    "# Affichage sous forme de tableau matplotlib\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis('off')\n",
    "table = ax.table(cellText=eps_df.values,\n",
    "                 colLabels=eps_df.columns,\n",
    "                 cellLoc='center',\n",
    "                 loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "fig.tight_layout()\n",
    "plt.title(\"R√©sum√© des eps estim√©s\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Boucle de test pour chaque couple (min_samples, eps)\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "for min_s, eps in eps_results:\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_s)\n",
    "    labels = dbscan.fit_predict(X)\n",
    "\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "\n",
    "    print(f\"min_samples = {min_s}, eps = {eps:.3f}, clusters = {n_clusters}, bruit = {n_noise} points\")\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='tab10', s=50)\n",
    "    plt.title(f\"DBSCAN (min_samples={min_s}, eps={eps:.3f})\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9de882",
   "metadata": {},
   "source": [
    "On voit ici que l'on choisit **min_samples = 7** et **eps = 0.136**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af622298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=0.136, min_samples=7)\n",
    "labels = dbscan.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='plasma', s=50)\n",
    "plt.title(\"Clustering DBSCAN\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914a538b",
   "metadata": {},
   "source": [
    "### üßæ 5.2 Pr√©diction de l'appartenance d'une donn√©e √† un cluster\n",
    "\n",
    "üß† **<u>Important</u>** : DBSCAN ne permet pas la pr√©diction directe (pas de .predict() üòî)\n",
    "mais on peut contourner ce probl√®me en :\n",
    "\n",
    "- entra√Ænant un **mod√®le de classification supervis√©e (ex : KNN en particulier)** sur les r√©sultats de DBSCAN,\n",
    "\n",
    "- puis en utilisant ce mod√®le pour pr√©dire l‚Äôappartenance d‚Äôun nouveau point.\n",
    "\n",
    "**<u>ATTENTION</u>** : on veillera √† **supprimer les donn√©es catalogu√©es comme \"bruit\"** qui fausseraient les r√©sultats : on les rep√®re gr√¢ce √† leur label valant -1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5b9f31",
   "metadata": {},
   "source": [
    "Comme le savez bien s√ªr üòÅ, l'algorithme des **plus proches voisins (KNN)** d√©pend du nombre de voisins pris en compte.\n",
    "Pour conna√Ætre leur nombre le plus adapt√©, on peut se servir du code suivant, indiquant son score pour chaque k-voisins sollicit√©s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f61b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Donn√©es tr√®s bruit√©es\n",
    "X, y = make_moons(n_samples=300, noise=0.25, random_state=42)\n",
    "\n",
    "\n",
    "# üìå Visualisation du nuage de points\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', s=50)\n",
    "plt.title(\"Donn√©es make_moons avec bruit important\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# üîç Validation crois√©e sur diff√©rentes valeurs de k\n",
    "scores = []\n",
    "k_values = range(1, 21)\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    score = cross_val_score(knn, X, y, cv=5).mean()\n",
    "    scores.append(score)\n",
    "\n",
    "    \n",
    "# üìà Affichage des scores\n",
    "plt.plot(k_values, scores, marker='o')\n",
    "plt.xlabel(\"Nombre de voisins (k)\")\n",
    "plt.ylabel(\"Score de validation crois√©e\")\n",
    "plt.title(\"Choix optimal de k pour KNN (donn√©es bruit√©es)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ‚úÖ Meilleur k\n",
    "best_k = k_values[np.argmax(scores)]\n",
    "print(f\"Meilleur k estim√© : {best_k} avec un score de {max(scores):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcfda87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On suppose le mod√®le pr√©c√©dent toujours entra√Æn√©\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# Suppression des points de bruit (-1)\n",
    "X_clean = X[labels != -1]\n",
    "y_clean = labels[labels != -1]\n",
    "\n",
    "\n",
    "# Entra√Ænement d'un classifieur KNN selon les \"k_neighbors\" sur les clusters trouv√©s\n",
    "k_neighbors = 9   # D'apr√®s pr√©c√©demment \n",
    "knn = KNeighborsClassifier(n_neighbors=k_neighbors)\n",
    "knn.fit(X_clean, y_clean)\n",
    "\n",
    "\n",
    "# Exemple : nouvelle donn√©e √† pr√©dire\n",
    "point_test = np.array([[1.0, 0.0]])\n",
    "cluster_pred = knn.predict(point_test)\n",
    "neighbors_idx = knn.kneighbors(point_test, return_distance=False)[0]\n",
    "neighbors_points = X_clean[neighbors_idx]\n",
    "\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(8, 5))\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.plasma(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "\n",
    "for label, color in zip(unique_labels, colors):\n",
    "    mask = labels == label\n",
    "    if label == -1:\n",
    "        color = (0, 0, 0, 1)  # noir pour le bruit\n",
    "        label_name = \"Bruit\"\n",
    "    else:\n",
    "        label_name = f\"Cluster {label}\"\n",
    "    plt.scatter(X[mask, 0], X[mask, 1], s=50, c=[color], label=label_name)\n",
    "\n",
    "    \n",
    "# Affichage du point test\n",
    "plt.scatter(point_test[0, 0], point_test[0, 1], c=\"cyan\", edgecolors=\"green\", s=200, marker=\"*\", label=f\"Point test (cluster {cluster_pred})\")\n",
    "\n",
    "# Colorier les voisins KNN\n",
    "plt.scatter(neighbors_points[:, 0], neighbors_points[:, 1], edgecolors=\"black\", facecolors=\"none\", s=200, linewidths=2, label=\"Voisins KNN\")\n",
    "\n",
    "\n",
    "plt.title(\"Visualisation DBSCAN avec pr√©diction d'un point test\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Le point {point_test[0]} est class√© dans le cluster {cluster_pred[0]}\")\n",
    "print(knn.predict_proba(point_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa947992",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß† 6. TP : DBSCAN sur donn√©es r√©elles\n",
    "\n",
    "Voici un exemple d'application de l‚Äôalgorithme **DBSCAN** √† des **donn√©es clients r√©alistes** sur deux crit√®res : le **salaire** et les **d√©penses** \n",
    "\n",
    "üìù **<u>Objectif</u>** : d√©tecter automatiquement des groupes de clients ayant des comportements similaires.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fc1b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "# Chargement des donn√©es\n",
    "df_clients = pd.read_csv(\"donnees_clients_dbscan.csv\")\n",
    "X = df_clients[[\"Revenu_Mensuel\", \"Depenses_Mensuelles\"]].values\n",
    "\n",
    "# Affichage des premi√®res lignes du jeu de donn√©es\n",
    "print(df_clients.head())\n",
    "\n",
    "\n",
    "\n",
    "# Choisir une plage de min_samples_range √† tester\n",
    "min_samples_range = range(2,10)\n",
    "eps_results = []  # Stocker les distances moyennes des plus grands √©carts visuels\n",
    "\n",
    "# Test pour chaque valeur de min_samples : le KNN permet d'estimer l'EPS\n",
    "################# A COMPLETER ####################\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Cr√©ation du tableau √† partir des r√©sultats\n",
    "eps_df = pd.DataFrame(eps_results, columns=[\"min_samples\", \"eps estim√©\"])\n",
    "print(eps_df)\n",
    "\n",
    "# Affichage sous forme de tableau matplotlib\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis('off')\n",
    "table = ax.table(cellText=eps_df.values,\n",
    "                 colLabels=eps_df.columns,\n",
    "                 cellLoc='center',\n",
    "                 loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "fig.tight_layout()\n",
    "plt.title(\"R√©sum√© des eps estim√©s\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Boucle de test pour chaque couple (min_samples, eps)\n",
    "for min_s, eps in eps_results:\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_s)\n",
    "    labels = dbscan.fit_predict(X)\n",
    "\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "\n",
    "    print(f\"min_samples = {min_s}, eps = {eps:.3f}, clusters = {n_clusters}, bruit = {n_noise} points\")\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='tab10', s=50)\n",
    "    plt.title(f\"DBSCAN (min_samples={min_s}, eps={eps:.3f})\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©termination du meilleur k pour le KNN\n",
    "scores = []\n",
    "k_values = range(1, 21)\n",
    "\n",
    "################# A COMPLETER ####################\n",
    "\n",
    "\n",
    "    \n",
    "# üìà Affichage des scores\n",
    "plt.plot(k_values, scores, marker='o')\n",
    "plt.xlabel(\"Nombre de voisins (k)\")\n",
    "plt.ylabel(\"Score de validation crois√©e\")\n",
    "plt.title(\"Choix optimal de k pour KNN (donn√©es bruit√©es)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ‚úÖ Meilleur k\n",
    "best_k = k_values[np.argmax(scores)]\n",
    "print(f\"Meilleur k estim√© : {best_k} avec un score de {max(scores):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783be4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Ænement du mod√®le DBSCAN avec le couple eps / min_samples d√©termin√©s ci-dessus    \n",
    "################# A COMPLETER ####################\n",
    "dbscan = DBSCAN(eps=75.9, min_samples=2)\n",
    "labels = dbscan.fit_predict(X)\n",
    "\n",
    "\n",
    "# Affichage des donn√©es (et clusters)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='plasma', s=50)\n",
    "plt.title(\"Clustering DBSCAN\")\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "# Suppression des points de bruit (-1)\n",
    "X_clean = X[labels != -1]\n",
    "y_clean = labels[labels != -1]\n",
    "\n",
    "\n",
    "# Entra√Ænement d'un classifieur KNN avec le meilleur k trouv√© pr√©c√©demment\n",
    "################# A COMPLETER ####################\n",
    "\n",
    "\n",
    "# Exemple : nouvelle donn√©e √† pr√©dire (√† l'aide de KNN)\n",
    "################# A COMPLETER ####################\n",
    "\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(8, 5))\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.plasma(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "\n",
    "for label, color in zip(unique_labels, colors):\n",
    "    mask = labels == label\n",
    "    if label == -1:\n",
    "        color = (0, 0, 0, 1)  # noir pour le bruit\n",
    "        label_name = \"Bruit\"\n",
    "    else:\n",
    "        label_name = f\"Cluster {label}\"\n",
    "    plt.scatter(X[mask, 0], X[mask, 1], s=50, c=[color], label=label_name)\n",
    "\n",
    "    \n",
    "# Affichage du point test\n",
    "plt.scatter(point_test[0, 0], point_test[0, 1], c=\"cyan\", edgecolors=\"green\", s=200, marker=\"*\", label=f\"Point test (cluster {cluster_pred})\")\n",
    "\n",
    "# Colorier les voisins KNN\n",
    "plt.scatter(neighbors_points[:, 0], neighbors_points[:, 1], edgecolors=\"black\", facecolors=\"none\", s=200, linewidths=2, label=\"Voisins KNN\")\n",
    "\n",
    "\n",
    "plt.title(\"Visualisation DBSCAN avec pr√©diction d'un point test\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Le point {point_test[0]} est class√© dans le cluster {cluster_pred[0]}\")\n",
    "print(knn.predict_proba(point_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6452ef7f",
   "metadata": {},
   "source": [
    "### üìå Travail √† faire / Questions :\n",
    "\n",
    "1. **Analyser les donn√©es** : visualiser le nuage de points et **estimer** le nombre de clusters.\n",
    "2. **Choisir une plage de `min_samples` √† tester**.\n",
    "3. Tester les r√©sultats de **DBSCAN** pour chaque couple (`min_samples`, `eps`).\n",
    "4. ‚úÖ Choisir les **meilleurs param√®tres** `eps` / `min_samples` selon :\n",
    "   - nombre de clusters d√©tect√©s selon la question 2.)\n",
    "   - nombre de points bruit√©s (le moins possible)\n",
    "   - coh√©rence des groupes\n",
    "   \n",
    "   \n",
    "5. **D√©terminer** le cluster d'appartenance d'un client gagnant 2000 euros et d√©pendant 1500 euros par mois.\n",
    "6. Quelles **am√©liorations** pourrait-on apporter √† ce mod√®le ? On observera la visualisation pour ce faire.\n",
    "---\n",
    "\n",
    "üí° **Aide** : s'inspirer de l‚Äôexemple pr√©c√©dent sur `make_moons`, copier-coller les cellules n√©cessaires en adaptant les donn√©es. On v√©rifiera √©galement que le meilleur couple `min_samples / eps` est *(2 , 75.9)* et que le meilleur `k` pour le KNN est *3* ou *4*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
